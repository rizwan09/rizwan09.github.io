---
title: "A Transformer-based Approach for Source Code Summarization"
collection: publications
Authors: '<b>Wasi Ahmad</b>, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.'
date: 04/2020
venue: 'ACL'
paperurl: 'https://wasiahmad.github.io/files/publications/2019/transformer_for_code_summ.pdf'
presentationurl: ''
codeurl: 'https://github.com/wasiahmad/NeuralCodeSum'
excerpt: ''
---
---
<a href='https://wasiahmad.github.io/files/publications/2020/' target="_blank">[Download Paper]</a>

<p align="justify">
Generating a readable summary that describes the functionality of a program is known as source code summarization.
Learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies are crucial for source code summarization.
A family of neural networks known as <em>Transformer</em> that uses a self-attention mechanism has shown to be effective in capturing long-range dependencies. 
In this work, we propose to leverage the Transformer for source code summarization and show that it outperforms the state-of-the-art approaches by a significant margin.
We perform extensive analysis and ablation studies to quantify the effect of several modeling alternatives (in multi-head attention and position representation) that will help to direct future research.
</p>
